import streamlit as st
import feedparser
import pandas as pd
from streamlit_autorefresh import st_autorefresh
from datetime import datetime
import urllib.parse
import pytz
import hashlib

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Global Tech News Hub", layout="wide")
st.title("ğŸ“¡ ì‹¤ì‹œê°„ ì™¸ì‹  í…Œí¬ ë‰´ìŠ¤ í—ˆë¸Œ")

# 60ì´ˆë§ˆë‹¤ í™”ë©´ ìë™ ê°±ì‹ 
st_autorefresh(interval=60000, key="news_refresh")

# ğŸ”¥ ì¤‘ë³µ ë°©ì§€ìš© ì „ì—­ ì €ì¥ì†Œ
if "seen_ids" not in st.session_state:
    st.session_state.seen_ids = set()

# --- ë¹„ë°€ë²ˆí˜¸ ì„¤ì •ë¶€ ---
def check_password():
    """ë¹„ë°€ë²ˆí˜¸ê°€ ë§ìœ¼ë©´ True, ì•„ë‹ˆë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    def password_entered():
        if st.session_state["password"] == st.secrets["APP_PASSWORD"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]  # ë³´ì•ˆì„ ìœ„í•´ ì„¸ì…˜ì—ì„œ ë¹„ë°€ë²ˆí˜¸ ì‚­ì œ
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ì°½ í‘œì‹œ
        st.text_input("ğŸ”‘ ì ‘ê·¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password", on_change=password_entered, key="password")
        return False
    elif not st.session_state["password_correct"]:
        # ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ì„ ë•Œ
        st.text_input("ğŸ”‘ ì ‘ê·¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password", on_change=password_entered, key="password")
        st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
        return False
    else:
        # ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì•˜ì„ ë•Œ
        return True

# ë¹„ë°€ë²ˆí˜¸ ì²´í¬ ì‹¤í–‰
if not check_password():
    st.stop()  # ë¹„ë°€ë²ˆí˜¸ê°€ ë§ê¸° ì „ê¹Œì§€ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ

# 2. ì¹´í…Œê³ ë¦¬ ì •ì˜ (CNBC ì „ìš© ì¹´í…Œê³ ë¦¬ ì¶”ê°€)
CATEGORIES = {
    "â­ ì´ˆì†ë³´ (Direct)": [
        "https://techcrunch.com/feed/",
        "https://www.theverge.com/rss/index.xml",
        "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=2000&keywords=technology",
        "https://9to5mac.com/feed/",
        "https://www.reutersagency.com/feed/?best-topics=technology&post_type=best",
        "https://www.zdnet.com/news/rss.xml"
    ],
    "ğŸ“º CNBC (Tech/Stock)": "CNBC_TECH_FILTER", # CNBC ì „ìš© í•„í„° ì˜ˆì•½ì–´
    "AI/NVIDIA": "NVIDIA OR NVDA OR 'Artificial Intelligence' OR Blackwell",
    "ë°˜ë„ì²´": "Semiconductor OR Chips OR TSMC OR ASML OR AVGO",
    "í…ŒìŠ¬ë¼/ë¨¸ìŠ¤í¬": "Tesla OR TSLA OR 'Elon Musk' OR Optimus",
    "ë¹…í…Œí¬": "Apple OR Microsoft OR Google OR Meta",
    "ì „ë ¥ ì¸í”„ë¼": "Data Center Energy OR Vertiv OR VRT OR NextEra",
    "ë¡œë³´í‹±ìŠ¤": "Robot OR Robotics OR Humanoid OR 'AI Robot' OR Automation OR Boston Dynamics OR Figure AI OR Optimus"
}

# 3. ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜
@st.cache_data(ttl=60)
def get_news_feed(category_name, source):
    news_list = []
    kst = pytz.timezone('Asia/Seoul')
    now_utc = datetime.now(pytz.utc)

    # --- Case 1: ì§ì ‘ RSS í”¼ë“œ (ì´ˆì†ë³´ ë¦¬ìŠ¤íŠ¸) ---
    if isinstance(source, list):
        for url in source:
            feed = feedparser.parse(url)
            for entry in feed.entries[:15]:
                try:
                    if hasattr(entry, 'published_parsed'):
                        dt_utc = datetime(*entry.published_parsed[:6], tzinfo=pytz.utc)
                    else:
                        dt_utc = pd.to_datetime(entry.published, utc=True)
                    
                    if (now_utc - dt_utc).total_seconds() > 21600: # 6ì‹œê°„
                        continue
                    
                    title = entry.title
                    item_id = hashlib.md5(title.encode()).hexdigest()[:12]
                    
                    if item_id in st.session_state.seen_ids: continue
                    st.session_state.seen_ids.add(item_id)

                    news_list.append({
                        "id": item_id, "category": category_name,
                        "time": dt_utc.astimezone(kst).strftime('%m/%d %H:%M'),
                        "title": title, "link": entry.link,
                        "source": urllib.parse.urlparse(url).netloc.replace('www.', ''),
                        "dt": dt_utc
                    })
                except: continue

   # --- Case 2: CNBC ì „ìš© í•„í„° ìˆ˜ì§‘ (ë³´ê°• ë²„ì „) ---
    elif source == "CNBC_TECH_FILTER":
        cnbc_rss_url = "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=2000&keywords=technology"
        feed = feedparser.parse(cnbc_rss_url)
        
        # í•„í„°ë§ í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥ (ë” ë§ì€ ë‰´ìŠ¤ í¬ì°©)
        tech_keywords = [
            "Tesla", "Musk", "Nvidia", "AI", "Apple", "Microsoft", "Google", "Meta", "Amazon", 
            "Semiconductor", "Chip", "OpenAI", "Blackwell", "SpaceX", "EV", "Earnings", "Fed", "Rate",
            "Broadcom", "TSMC", "ASML", "Intelligence", "Computing", "Software"
        ]
        
        for entry in feed.entries[:50]: # ë” ë§ì€ ê¸°ì‚¬ë¥¼ í›‘ì–´ë´…ë‹ˆë‹¤.
            try:
                title = entry.title
                # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ í‚¤ì›Œë“œ ë§¤ì¹­
                if not any(kw.lower() in title.lower() for kw in tech_keywords):
                    continue
                    
                dt_utc = pd.to_datetime(entry.published, utc=True)
                # 24ì‹œê°„(86400ì´ˆ) ì´ë‚´ ê¸°ì‚¬ê¹Œì§€ í—ˆìš©í•˜ì—¬ ê³µë°±ê¸° ë°©ì§€
                if (now_utc - dt_utc).total_seconds() > 86400:
                    continue

                item_id = hashlib.md5(title.encode()).hexdigest()[:12]
                if item_id in st.session_state.seen_ids: continue
                st.session_state.seen_ids.add(item_id)

                news_list.append({
                    "id": item_id, "category": category_name,
                    "time": dt_utc.astimezone(kst).strftime('%m/%d %H:%M'),
                    "title": title, "link": entry.link,
                    "source": "CNBC", "dt": dt_utc
                })
            except: continue

    # --- Case 3: Google News ê²€ìƒ‰ ---
    else:
        encoded_query = urllib.parse.quote(f"{source} when:1h")
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        feed = feedparser.parse(url)
        for entry in feed.entries[:30]:
            try:
                dt_utc = pd.to_datetime(entry.published, utc=True)
                if (now_utc - dt_utc).total_seconds() > 3600: continue

                full_title = entry.title
                title_part = full_title.rsplit(' - ', 1)[0] if ' - ' in full_title else full_title
                source_part = entry.source.title if hasattr(entry, 'source') else "Google News"
                item_id = hashlib.md5(title_part.encode()).hexdigest()[:12]

                if item_id in st.session_state.seen_ids: continue
                st.session_state.seen_ids.add(item_id)

                news_list.append({
                    "id": item_id, "category": category_name,
                    "time": dt_utc.astimezone(kst).strftime('%m/%d %H:%M'),
                    "title": title_part, "link": entry.link,
                    "source": source_part, "dt": dt_utc
                })
            except: continue

    return sorted(news_list, key=lambda x: x['dt'], reverse=True)

# 4. ìƒë‹¨ ê³µí†µ ì•ˆë‚´
st.info("ğŸ’¡ **ì´ìš© ê°€ì´ë“œ**: 'ì´ˆì†ë³´'ì™€ 'CNBC' íƒ­ì€ RSSë¥¼ ì§ì ‘ ìˆ˜ì‹ í•˜ë©°, ë‚˜ë¨¸ì§€ëŠ” Google ê²€ìƒ‰ 1ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ì…ë‹ˆë‹¤.")

# 5. ìƒë‹¨ íƒ­ êµ¬ì„±
tabs = st.tabs(list(CATEGORIES.keys()))

# ğŸ”¥ ìƒˆë¡œê³ ì¹¨ ì‹œ ì¤‘ë³µ ì´ˆê¸°í™”
st.session_state.seen_ids = set()

# 6. ê° íƒ­ë³„ ë‰´ìŠ¤ ì¶œë ¥ ë£¨í”„
for tab_idx, (tab, (cat_name, source)) in enumerate(zip(tabs, CATEGORIES.items())):
    with tab:
        news_data = get_news_feed(cat_name, source)
        now_kst = datetime.now(pytz.timezone('Asia/Seoul')).strftime('%H:%M:%S')

        if news_data:
            df = pd.DataFrame(news_data)
            st.caption(f"ğŸ”¥ í˜„ì¬ **{len(df)}ê°œ**ì˜ ìµœì‹  ë‰´ìŠ¤ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. (ë§ˆì§€ë§‰ ê°±ì‹ : {now_kst})")

            for i, (_, row) in enumerate(df.iterrows()):
                widget_key = f"copy_{tab_idx}_{i}_{row['id']}"

                with st.container():
                    col1, col2 = st.columns([3, 1.2])
                    with col1:
                        st.markdown(f"### {row['title']}")
                        st.caption(f"ğŸ•’ {row['time']} | ì¶œì²˜: {row['source']}")
                        st.link_button(f"ğŸ“„ {row['source']} ì›ë¬¸ ê¸°ì‚¬ ë³´ê¸°", row['link'])

                    with col2:
                        prompt_text = (
                            f"ì¶œì²˜ê°€ '{row['source']}'ì¸ '{row['title']}' ê¸°ì‚¬ë¥¼ ì°¾ì•„ì„œ ë‹¤ìŒ ìˆœì„œë¡œ ë‹µí•´ì¤˜:\n\n"
                            f"1. **ê¸°ì‚¬ ì „ë¬¸ ë²ˆì—­ ë° ìƒì„¸ ìš”ì•½**\n"
                            f"   - ê¸°ì‚¬ ì „ì²´ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê²Œ ë²ˆì—­\n"
                            f"   - í•µì‹¬ ë‚´ìš©ì„ ë†“ì¹¨ ì—†ì´ ìì„¸í•˜ê²Œ ìš”ì•½\n\n"
                            f"2. **êµ­ì™¸(ê¸€ë¡œë²Œ) ì£¼ì‹ ì‹œì¥ ì—°ê´€ì„±**\n"
                            f"   - í•´ë‹¹ ì†Œì‹ìœ¼ë¡œ ì˜í–¥ì„ ë°›ëŠ” ë¯¸êµ­ ë“± í•´ì™¸ ì£¼ìš” ì¢…ëª©ê³¼ ì„¹í„° ë¶„ì„\n\n"
                            f"3. **êµ­ë‚´ ì£¼ì‹ ì‹œì¥ ì—°ê´€ì„±**\n"
                            f"   - êµ­ë‚´ ì‹œì¥ì—ì„œë„ ì˜í–¥ì´ ìˆì„ì§€ ì—¬ë¶€ì™€ êµ¬ì²´ì ì¸ ì´ìœ \n"
                            f"   - ì—°ê´€ëœ êµ­ë‚´ ì£¼ì‹ ì¢…ëª©(ìˆ˜í˜œì£¼/í”¼í•´ì£¼)ê³¼ ê´€ë ¨ í…Œë§ˆ(ì˜ˆ: HBM, ììœ¨ì£¼í–‰ ë“±)\n\n"
                            f"4. **íˆ¬ìì ê´€ì ì˜ ìµœì¢… ê²°ë¡ **\n"
                            f"   - ì´ ê¸°ì‚¬ê°€ ì‹œì¥ì— ì£¼ëŠ” ì‹œê·¸ë„ ìš”ì•½ ë° íˆ¬ì ë§¤ë ¥ë„ ë¶„ì„"
                        )
                        st.text_area("ëª…ë ¹ì–´ ë³µì‚¬ (Ctrl+C)", value=prompt_text, height=150, key=widget_key)
                        st.link_button("ğŸ¤– Gemini ì—´ê¸°", "https://gemini.google.com/app", type="primary", use_container_width=True)
                    st.divider()
        else:
            st.warning(f"í˜„ì¬ '{cat_name}' ì¹´í…Œê³ ë¦¬ì— ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
