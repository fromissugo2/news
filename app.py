import streamlit as st
import feedparser
import pandas as pd
from streamlit_autorefresh import st_autorefresh
from datetime import datetime
import urllib.parse
import pytz

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Global Tech News Hub", layout="wide")
st.title("ğŸ“¡ ì‹¤ì‹œê°„ ì™¸ì‹  í…Œí¬ ë‰´ìŠ¤ í—ˆë¸Œ")

st_autorefresh(interval=60000, key="news_refresh")

# 2. ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES = {
    "AI": "AI OR Artificial Intelligence",
    "ë°˜ë„ì²´": "Semiconductor OR Chips",
    "ì—”ë¹„ë””ì•„": "NVIDIA OR NVDA",
    "í…ŒìŠ¬ë¼": "Tesla OR TSLA",
    "ì¼ë¡  ë¨¸ìŠ¤í¬": '"Elon Musk"'
    "AI/NVIDIA": "NVIDIA OR NVDA OR Blackwell",
    "ë¹…í…Œí¬": "Tesla OR Apple OR Microsoft OR Google",
    "ì „ë ¥ ì¸í”„ë¼": "Data Center Energy OR Vertiv OR VRT",
    "ë°˜ë„ì²´": "Broadcom OR AVGO OR TSMC",
    "ë¡œë³´í‹±ìŠ¤": "Humanoid Robot OR Tesla Optimus OR Figure AI"
}

# 3. ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (ê°€ì¥ ê¸°ë³¸ì ì´ê³  ë¹ ë¥¸ RSS ìˆ˜ì§‘)
def get_news_feed(category_name, query):
    encoded_query = urllib.parse.quote(f"{query} when:1h")
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(url)
    news_list = []
    kst = pytz.timezone('Asia/Seoul')
    
    if hasattr(feed, 'entries'):
        for entry in feed.entries[:8]:
            try:
                dt_utc = pd.to_datetime(entry.published, utc=True)
                # ì œëª©ì—ì„œ ì¶œì²˜( - Source) ë¶€ë¶„ ë¶„ë¦¬
                full_title = entry.title
                title_part = full_title.rsplit(' - ', 1)[0] if ' - ' in full_title else full_title
                source_part = entry.source.title if hasattr(entry, 'source') else "News Source"
                
                news_list.append({
                    "category": category_name,
                    "time": dt_utc.astimezone(kst).strftime('%m/%d %H:%M'),
                    "title": title_part,
                    "google_link": entry.link,
                    "source": source_part,
                    "dt": dt_utc
                })
            except: continue
    return news_list

# 4. ë‰´ìŠ¤ ì‹¤í–‰ ë° ì¶œë ¥
all_news = []
for cat_name, query in CATEGORIES.items():
    all_news.extend(get_news_feed(cat_name, query))

if all_news:
    df = pd.DataFrame(all_news).drop_duplicates(subset=['title']).sort_values(by="dt", ascending=False)
    
    st.info("âœ… 'Gemini ì—´ê¸°' í´ë¦­ ì‹œ, í•´ë‹¹ ê¸°ì‚¬ë¥¼ Geminiê°€ ì§ì ‘ ì°¾ì•„ ë²ˆì—­í•˜ë„ë¡ ëª…ë ¹ì–´ê°€ ìë™ êµ¬ì„±ë©ë‹ˆë‹¤.")

    for i, row in df.iterrows():
        with st.container():
            col1, col2 = st.columns([3, 1.2])
            
            with col1:
                st.markdown(f"### <{row['category']}> {row['title']}")
                st.caption(f"ğŸ•’ {row['time']} | ì¶œì²˜: {row['source']}")
                # ì›ë¬¸ ë³´ê¸°ëŠ” êµ¬ê¸€ ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ì“°ë˜, ìƒˆ íƒ­ì—ì„œ ì—´ë¦¬ë„ë¡ í•¨
                st.link_button("ğŸ“„ ì›ë¬¸ ê¸°ì‚¬ ì§ì ‘ ë³´ê¸°", row['google_link'])
            
            with col2:
                # [í•´ê²°ì±…] ë§í¬ ëŒ€ì‹  'ì œëª©'ê³¼ 'ì¶œì²˜'ë¥¼ ì¡°í•©í•´ Geminiì—ê²Œ ë˜ì§‘ë‹ˆë‹¤.
                # ì´ë ‡ê²Œ í•˜ë©´ Geminiê°€ ìì‹ ì˜ ê²€ìƒ‰ ëŠ¥ë ¥ì„ ì‚¬ìš©í•´ ì •í™•í•œ ê¸°ì‚¬ë¥¼ ì°¾ì•„ë‚´ì–´ ë²ˆì—­í•©ë‹ˆë‹¤.
                prompt_text = f"ì¶œì²˜ê°€ '{row['source']}'ì¸ '{row['title']}' ê¸°ì‚¬ë¥¼ ì°¾ì•„ì„œ í•œêµ­ì–´ë¡œ ì „ë¬¸ ë²ˆì—­í•˜ê³  ìì„¸íˆ ìš”ì•½í•´ì¤˜."
                
                st.text_area("ëª…ë ¹ì–´ ë³µì‚¬ (Ctrl+C)", value=prompt_text, height=90, key=f"copy_{i}")
                st.link_button("ğŸ¤– Gemini ì—´ê¸°", "https://gemini.google.com/app", type="primary", use_container_width=True)
            
            st.divider()
